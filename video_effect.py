import cv2
import mediapipe as mp
import numpy as np

# --- Khá»Ÿi táº¡o Mediapipe ---
mp_selfie_segmentation = mp.solutions.selfie_segmentation
segment = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)

# --- Video input/output ---
input_path = "output_video_with_music.mp4"
output_path = "output_effect.mp4"

cap = cv2.VideoCapture(input_path)
if not cap.isOpened():
    print("KhÃ´ng má»Ÿ Ä‘Æ°á»£c video!")
    exit()

width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps    = cap.get(cv2.CAP_PROP_FPS) or 30.0

out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

# --- Cáº¥u hÃ¬nh viá»n trÃ²n ---
max_radius = 2000       # bÃ¡n kÃ­nh lá»›n nháº¥t
radius_step = 40        # tá»‘c Ä‘á»™ lan tá»a má»—i frame
circle_color = (0, 140, 255)  # mÃ u cam BGR
thickness = 8           # Ä‘á»™ dÃ y viá»n

# --- XÃ¡c Ä‘á»‹nh vá»‹ trÃ­ ngÆ°á»i ---
ret, frame = cap.read()
if not ret:
    print("KhÃ´ng Ä‘á»c Ä‘Æ°á»£c video!")
    exit()

frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
result = segment.process(frame_rgb)
mask_init = (result.segmentation_mask > 0.3).astype(np.uint8) * 255

ys, xs = np.where(mask_init > 0)
if len(xs) == 0 or len(ys) == 0:
    print("KhÃ´ng phÃ¡t hiá»‡n Ä‘Æ°á»£c ngÆ°á»i trong khung hÃ¬nh")
    exit()

x_center = int(np.mean(xs))
y_top = int(np.min(ys))
y_bottom = int(np.max(ys))
y_chest = int(y_top + 0.5 * (y_bottom - y_top))
center = (x_center, y_chest)

cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

current_radius = 0
radius_growing = True

# --- Xá»­ lÃ½ tá»«ng frame ---
while True:
    ret, frame = cap.read()
    if not ret:
        break

    # --- TÃ­nh thá»i gian hiá»‡n táº¡i cá»§a video (ms) ---
    current_time_ms = cap.get(cv2.CAP_PROP_POS_MSEC)
    current_time_sec = current_time_ms / 1000.0

    video_duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / fps
    start_time = video_duration / 2
    if current_time_sec < start_time:
        out.write(frame)
        continue


    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = segment.process(frame_rgb)
    mask_person = (result.segmentation_mask > 0.3).astype(np.uint8) * 255

    # --- Táº¡o viá»n "Ä‘iá»‡n giáº­t" hÃ¬nh chá»¯ nháº­t dá»c ---
    rect_mask = np.zeros((height, width), dtype=np.uint8)

    # KÃ­ch thÆ°á»›c cÆ¡ báº£n cá»§a khung Ä‘iá»‡n giáº­t
    rect_width = int(20000)   # Ä‘iá»u chá»‰nh tá»· lá»‡ chiá»u ngang
    rect_height = int(current_radius * 1.6)  # Ä‘iá»u chá»‰nh tá»· lá»‡ chiá»u dá»c

    # Táº¡o mÃ©p chá»¯ nháº­t (4 cáº¡nh)
    amplitude = 15      # Ä‘á»™ rung Ä‘iá»‡n
    smoothness = 20     # máº­t Ä‘á»™ dao Ä‘á»™ng cáº¡nh
    phase = cv2.getTickCount() / cv2.getTickFrequency() * 4.0

    # Táº¡o cáº¡nh trÃªn & dÆ°á»›i
    x = np.linspace(-rect_width // 2, rect_width // 2, 200)
    noise_top = np.sin(x / 10 + phase) * amplitude + np.random.uniform(-5, 5, size=x.shape)
    noise_bottom = np.sin(x / 8 + phase + np.pi) * amplitude + np.random.uniform(-5, 5, size=x.shape)

    top_pts = np.stack((center[0] + x, center[1] - rect_height // 2 + noise_top), axis=1).astype(np.int32)
    bottom_pts = np.stack((center[0] + x, center[1] + rect_height // 2 + noise_bottom), axis=1).astype(np.int32)

    # Táº¡o cáº¡nh trÃ¡i & pháº£i
    y = np.linspace(-rect_height // 2, rect_height // 2, 200)
    noise_left = np.sin(y / 12 + phase) * amplitude + np.random.uniform(-5, 5, size=y.shape)
    noise_right = np.sin(y / 9 + phase + np.pi / 2) * amplitude + np.random.uniform(-5, 5, size=y.shape)

    left_pts = np.stack((center[0] - rect_width // 2 + noise_left, center[1] + y), axis=1).astype(np.int32)
    right_pts = np.stack((center[0] + rect_width // 2 + noise_right, center[1] + y), axis=1).astype(np.int32)

    # Gá»™p 4 cáº¡nh láº¡i thÃ nh khung Ä‘iá»‡n
    pts = np.concatenate([top_pts, right_pts, bottom_pts[::-1], left_pts[::-1]]).reshape((-1, 1, 2))

    # Váº½ viá»n Ä‘iá»‡n giáº­t
    cv2.polylines(rect_mask, [pts], isClosed=True, color=255, thickness=thickness, lineType=cv2.LINE_AA)

    # --- Giá»¯ pháº§n viá»n trÃªn ngÆ°á»i ---
    visible_ring = cv2.bitwise_and(rect_mask, mask_person)

    # --- Ãp mÃ u viá»n chÃ­nh ---
    frame_out = frame.copy()
    frame_out[visible_ring > 0] = circle_color

    # --- Glow (phÃ¡t sÃ¡ng máº¡nh kiá»ƒu LED) ---
    glow_base = visible_ring.copy()

    # 3 lá»›p bloom
    glow1 = cv2.GaussianBlur(glow_base, (0, 0), 15)
    glow2 = cv2.GaussianBlur(glow_base, (0, 0), 35)
    glow3 = cv2.GaussianBlur(glow_base, (0, 0), 60)

    # TÃ´ mÃ u glow (Ä‘á»â€“cam rá»±c)
    def colorize(mask, r, g, b):
        c = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
        c[..., 2] = mask * r
        c[..., 1] = mask * g
        c[..., 0] = mask * b
        return c

    glow1 = colorize(glow1, 1.0, 0.5, 0.2)
    glow2 = colorize(glow2, 1.3, 0.4, 0.1)
    glow3 = colorize(glow3, 1.6, 0.3, 0.0)

    # TÄƒng cÆ°á»ng â€“ blend tá»«ng lá»›p
    frame_out = cv2.addWeighted(frame_out, 1.0, glow1, 0.6, 0)
    frame_out = cv2.addWeighted(frame_out, 1.0, glow2, 0.5, 0)
    frame_out = cv2.addWeighted(frame_out, 1.0, glow3, 0.45, 0)

    out.write(frame_out)

    # --- Cáº­p nháº­t bÃ¡n kÃ­nh ---
    if radius_growing:
        current_radius += radius_step
        if current_radius >= max_radius:
            current_radius = max_radius
            radius_growing = False

cap.release()
out.release()
segment.close()

# --- GhÃ©p láº¡i Ã¢m thanh gá»‘c báº±ng ffmpeg ---
import subprocess

final_output = "output_effect_with_audio.mp4"

cmd = [
    "ffmpeg",
    "-y",
    "-i", output_path,        # video Ä‘Ã£ xá»­ lÃ½ (khÃ´ng cÃ³ tiáº¿ng)
    "-i", input_path,         # video gá»‘c (cÃ³ tiáº¿ng)
    "-c:v", "copy",           # giá»¯ nguyÃªn video Ä‘Ã£ xá»­ lÃ½
    "-c:a", "aac",            # copy Ã¢m thanh AAC
    "-map", "0:v:0",          # láº¥y video tá»« file 0
    "-map", "1:a:0",          # láº¥y audio tá»« file 1
    final_output
]
subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

print("ğŸ§ Video cÃ³ Ã¢m thanh Ä‘Ã£ lÆ°u:", final_output)

print("ğŸ”¥ Video Ä‘Ã£ lÆ°u:", output_path)
